{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h1>Few-shot Learning</h1>\n",
    "\n",
    "<p>Few-shot learning is a machine learning paradigm that enables a model to learn from only a small number of examples. To achieve this, few-shot learning algorithms use prior knowledge learned from other tasks and apply it to new tasks with limited training data. One popular approach to few-shot learning is prototypical networks, which were proposed by Snell et al. in <a href='https://arxiv.org/abs/1703.05175'>this</a> research paper. </p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Outline</h2>\n",
    "<ul>\n",
    "    <li> Motivation: why few-shot learning is important </li>\n",
    "    <li> Meta Learning: few-shot learning is a type of meta-learning </li>\n",
    "    <li> Few-shot Learning: how to set up a few-shot learning in theory </li>\n",
    "    <li> Implementation: implement Prototypical Networks to gain a better understanding of demonstrate how few-shot learning works in practice </li>\n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Motivation</h2>\n",
    "<p>When building a classifier, the general steps are collecting large number of labeled samples, training the model over them, and finally, test the model on a test set to see how well the model is generalized to unseen data. This paradaigm is not a good choice in the following scenarios:</p>\n",
    "<ul>\n",
    "    <li>A small dataset (e.g. a few examples of plant species) </li>\n",
    "    <li>A large dataset with a long tail; that is, despite having a large dataset, for some classes, we only have a few examples</li>\n",
    "    <ul>\n",
    "        <li>Think about a large dataset of plants, for rare species, we only have a few examples. </li>\n",
    "        <li>A database of user-searched keywords, some keywords might have been searched only 3,4 times</li>\n",
    "    </ul>\n",
    "</ul>\n",
    "<p>When we have a small dataset, the model does not see enough variations of data features during training. Therefore, there might exist many combinations that the model has not seen before and consequently, it cannot be generalized well to unseen data.</p>\n",
    "<p>One approach when dealing with the above issues is using pretrained networks. However, finetuning over a small dataset can impact the model accuracy and also lead to severe overfitting.</p>\n",
    "<p>Few-shot learning aims to provide a solution when having not large enough dataset. In few-shot learning, the goal is to learn how to learn a task rather than learning the task itself. From this perspective, few-shot learning is a type of meta-learning.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Meta Learning: Learning to Learn!</h2>\n",
    "<p>\n",
    "In traditional machine learning, a model is trained to perform a specific task, such as classifying images into a set of predefined categories. However, meta learning takes a different approach: it aims to teach the model how to learn, rather than just how to perform a specific task.\n",
    "\n",
    "To accomplish this, meta learning involves training the model on multiple episodes of related tasks. In each episode, the model is presented with a new classification task and must adapt its parameters and strategies to perform well on that task. By training on a diverse set of tasks, the model can learn to quickly adapt to new tasks in the future.\n",
    "\n",
    "Compared to traditional classification problems, meta learning typically involves working with sets of tasks rather than sets of data. This approach is often referred to as few-shot learning, as the model is trained to learn from only a few examples of each task.\n",
    "\n",
    "Overall, meta learning is a powerful approach to machine learning that can lead to more flexible and adaptable models. By teaching models how to learn, we can build AI systems that are better equipped to handle new challenges and tasks.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Few-shot learning explained</h2>\n",
    "<p>In a traditional classification problem, the model is trained to predict a class or label for a given sample image. During inference, the model can only classify images into the same set of classes that it was trained on, and cannot generalize to new classes. For example, if a model is trained to classify images as 'car' or 'airplane', it will not be able to classify a 'ship' image correctly, as it has not seen any 'ship' images before.\n",
    "\n",
    "Few-shot learning takes a different approach, aiming to enable the model to generalize to unseen classes. In a few-shot learning scenario, the model is trained to learn how to learn a task, rather than just memorizing specific classes. To achieve this, the model is trained on multiple small classification tasks, each with a few labeled samples. For instance, given a dataset of images of cats, dogs, horses, and sheep, one task could be to classify cats vs. dogs, another could be dogs vs. horses, and so on.\n",
    "\n",
    "By repeatedly training on a diverse set of classification tasks, the model learns to adapt to new tasks quickly and generalize to new classes. In the example above, if the model is trained on a few labeled images of ships, it should be able to classify a 'ship' image correctly, even though it was not explicitly trained on the 'ship' class.\n",
    "\n",
    "Overall, few-shot learning is a powerful technique for enabling models to learn how to learn and generalize to new classes, which can lead to more flexible and adaptable AI systems.\n",
    "\n",
    " </p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Few-shot learning setting: N-way-k-shot classification </h2>\n",
    "<p>\n",
    "    To train our model, we need to create episodes where it can learn to perform several tasks. Each task is essentially a small classification problem. To define a task, we first randomly sample $n$ classes (known as $n$-way) from our dataset. For example, let's say we choose the \"dog\" and \"cat\" classes from a set of samples that includes \"dog\", \"cat\", \"horse\", and \"sheep\". For each of the n classes, we then collect two sets of data: $k$ images to act as the training set (known as the support set), and $q$ images to act as the test set (known as the query set). \n",
    "    \n",
    "   \n",
    "<ul>\n",
    "    <li>$k$ number (k-shots) of images to play the role of the training set. This set of images is called support set.</li>\n",
    "    <li> $q$ number of images are sampled. This set of images is called query set. The query set plays the role of the test set.</li>\n",
    "</ul>\n",
    "</p>\n",
    "<p> Here are some notes to remember:\n",
    "    <ul>\n",
    "        <li>An episode consists of multiple tasks.</li>\n",
    "        <li>A few-shot learning problem with the above setting is known as $N$-way-$k$-shot classification.</li>\n",
    "        <li>The value of $k$ (the number of shots or the size of the support set) is typically small - for example, just 5 -  because our ultimate goal is for the model to be able to accurately predict the class (which has not seen during the training) using only a few labeled samples from the support set.</li> \n",
    "    </ul>\n",
    "     \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Prototypical Networks</h2>\n",
    "<p>Prototypical networks were proposed by Snell et al. as a solution to the problem of few-shot learning. At their core, prototypical networks are a type of representation learning. They learn a metric space in which the classification task can be performed. Before we proceed any further, let's take a moment to define what we mean by a metric space. \n",
    "</p>\n",
    "<h3>Metric Space</h3>\n",
    "<p>\n",
    "A metric space is a collection of points, where we can define a metric, or in other words, a measure of distance between any two points. By creating an embedding or metric space, prototypical networks enable few-shot learning by allowing the classification of new data points based on their similarity to a few labeled examples.\n",
    "</p>\n",
    "<p>\n",
    "    In mathematics, a metric space $X$ is a set of points over which we can define a metric $d$ or a distance function. $d$ is a function defined as $d: X\\times X \\rightarrow \\mathbb R$  that satisfies three conditions for all $x, y, z \\in X$:\n",
    "    $$d(x,y) = 0 \\Longleftrightarrow x=y$$\n",
    "    $$d(x,y) = d(y,x)$$\n",
    "    $$d(x,y) \\le d(x,z) + d(z,y) $$\n",
    "These conditions describe three fundamental properties of a metric space. First, the distance between a point and itself is zero. Second, the distance between two points, $x$ and $y$, is the same regardless of whether we start from x or y. Finally, the distance between two points via a direct path is less than or equal to the distance between those points if we pass through an intermediate point. \n",
    "</p>\n",
    "<p>\n",
    "With a clear definition for distance in hand, we can now define a measure of similarity between points. Simply put, the closer two points are to each other based on the distance metric we have defined, the more similar they are considered to be.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>The core idea in prototypical networks </h3>\n",
    "<p>\n",
    "    Prototypical networks work under the assumption that there is a representation space in which all the points that belong to the same class gather around a common prototype. To achieve this, the network uses a non-linear mapping of the input to create an embedding space. The prototype for each class is defined as the mean of the support set in the embedding space. To classify a new data point, the network calculates its embedding and assigns it to the nearest prototype. This approach enables the network to perform few-shot learning by using a small support set of labeled examples to create prototypes that can classify new, unlabeled data points.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Build a prototypical network</h3>\n",
    "<p>\n",
    " A prototypical network needs a CNN network as the backbone. This CNN network maps every image to a feature vector or an embedding vector. In other words, every image is mapped to a point in the metric space that the prototypical network has learnt (or is learning if we are still in the training phase). These embeddigs are used then to see how similar or disimilar are the images. Inputs of the network are images from support sets and query sets.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The first step is defining an episode. In other words, we must sample a set of classes, and break down the dataset into the support sets and the query sets. Please note that similar to the training phase of every deep learning model being performed iteratively through different epochs, training of a prototypical model is performed iteratively through multiple episodes. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Data Preperation </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>To demonstrate how to build, train, and test a prototypical network, I used the <a href='https://github.com/brendenlake/omniglot'>Omniglot dataset</a>. This dataset comprises handwritten images of 1623 characters from 50 different alphabets. As stated by the authors, the dataset is split into a background set containing 30 alphabets, which is used for training, and an evaluation set containing 20 alphabets, which is used for evaluation.\n",
    "    </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.models import resnet18\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import Omniglot\n",
    "from matplotlib import pyplot as plt\n",
    "from random import sample\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# /Users/lidasafarnejad/PycharmProjects/Few-Shot-Learning/Prototypical_network.ipynb\n",
    "# /Users/lidasafarnejad/PycharmProjects/Few-Shot-Learning/data\n",
    "# data/\n",
    "# Prototypical_network.ipynb\n",
    "image_size = 28\n",
    "train_set = Omniglot(\n",
    "    root= 'data',\n",
    "    transform=transforms.Compose(\n",
    "        [\n",
    "            transforms.Grayscale(num_output_channels=3),\n",
    "            transforms.RandomResizedCrop(image_size),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor()\n",
    "\n",
    "        ]\n",
    "    ),\n",
    "    background=True,\n",
    "    download=False\n",
    ")\n",
    "\n",
    "test_set = Omniglot(\n",
    "    root = 'data',\n",
    "    background=False,\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "        transforms.Grayscale(num_output_channels = 3),\n",
    "        transforms.Resize([int(image_size * 1.15), int(image_size * 1.15)]),\n",
    "            transforms.CenterCrop(image_size),\n",
    "            transforms.ToTensor(),\n",
    "\n",
    "        ]\n",
    "    ),\n",
    "    download=False\n",
    ")\n",
    "unique_labels = set()\n",
    "for im in train_set._flat_character_images:\n",
    "    unique_labels.add(im[1])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Here is one sample image:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_set[1][0].permute(1, 2, 0) )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>I also customized the dataset into a dictionary to sample the classes and the images more easily. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Organize data in dictionary to be used for sampling.\n",
    "# The keys are classes\n",
    "data_dict = {}\n",
    "for x in train_set:\n",
    "    if x[1] not in data_dict:\n",
    "        data_dict[x[1]] = []\n",
    "    data_dict[x[1]].append(x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Preparing episodes</h3>\n",
    "<p>Now that we have the dataset ready, we must prepare episods. That is, we must sample a set of classes, and break down the dataset into the support sets and the query sets. Please note that similar to the training phase of every deep learning model being performed iteratively through different epochs, training of a prototypical model is performed iteratively through multiple episodes. </p>\n",
    "<p>The custom collate function accepts the number of ways (classes), the number of shots (the size of the support set), and the number of the quary set as parameters and returns the support set, the quary set and their labels.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate(n_way,n_shot,n_query):\n",
    "    \"\"\"\n",
    "\n",
    "    :param n_way: number of classes in each episode\n",
    "    :param n_shot: size of the support set\n",
    "    :param n_query: size of the quary set\n",
    "    :return:\n",
    "            support_images: torch tensor\n",
    "            quary_images: tocrh tensor\n",
    "            support_labels: torch tensor\n",
    "            quary_labels: torch tensor\n",
    "            classes: torch tensor\n",
    "            true_support_labels: torch tensor\n",
    "            true_quary_labels: torch tensor\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    classes = sample(labels, n_way)\n",
    "    support_images = torch.empty(n_way*n_shot*3*28*28).reshape((n_way,n_shot,3,28,28))\n",
    "    quary_images = torch.empty(n_way*n_quary*3*28*28).reshape((n_way,n_quary,3,28,28))\n",
    "    support_labels = []\n",
    "    quary_labels = []\n",
    "    true_support_labels = []\n",
    "    true_quary_labels = []\n",
    "    for i in range(n_way):\n",
    "        images_per_class=sample(data_dict[classes[i]],n_shot+n_query)\n",
    "        # plt.imshow(images_per_class[:5][4].permute(1, 2, 0))\n",
    "        # plt.show()\n",
    "        support_images[i] = torch.stack(images_per_class[:n_shot])\n",
    "\n",
    "        quary_images[i] = torch.stack(images_per_class[n_shot:])\n",
    "        support_labels = support_labels + (n_shot) * [i]\n",
    "        quary_labels = quary_labels + (n_quary) * [i]\n",
    "        true_support_labels = true_support_labels + (n_shot)*[classes[i]]\n",
    "        true_quary_labels = true_quary_labels + (n_quary)*[classes[i]]\n",
    "\n",
    "    support_labels = torch.tensor(support_labels)\n",
    "    quary_labels = torch.tensor(quary_labels)\n",
    "    true_support_labels = torch.tensor(true_support_labels)\n",
    "    true_quary_labels = torch.tensor(true_quary_labels)\n",
    "\n",
    "    return support_images,quary_images,support_labels,quary_labels,classes,true_support_labels,true_quary_labels\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seq2seqvenv",
   "language": "python",
   "name": "seq2seqvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
