{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h1>Few-shot Learning</h1>\n",
    "\n",
    "<p>Few-shot learning is a machine learning paradigm that enables a model to learn from only a small number of examples. To achieve this, few-shot learning algorithms use prior knowledge learned from other tasks and apply it to new tasks with limited training data. One popular approach to few-shot learning is prototypical networks, which were proposed by Snell et al. in <a href='https://arxiv.org/abs/1703.05175'>this</a> research paper. </p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Outline</h2>\n",
    "<ul>\n",
    "    <li> Motivation: why few-shot learning is important </li>\n",
    "    <li> Meta Learning: few-shot learning is a type of meta-learning </li>\n",
    "    <li> Few-shot Learning: how to set up a few-shot learning in theory </li>\n",
    "    <li> Implementation: implement Prototypical Networks to gain a better understanding of demonstrate how few-shot learning works in practice </li>\n",
    "    </ul>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Motivation</h2>\n",
    "<p>When building a classifier, the general steps are collecting large number of labeled samples, training the model over them, and finally, test the model on a test set to see how well the model is generalized to unseen data. This paradaigm is not a good choice in the following scenarios:</p>\n",
    "<ul>\n",
    "    <li>A small dataset (e.g. a few examples of plant species) </li>\n",
    "    <li>A large dataset with a long tail; that is, despite having a large dataset, for some classes, we only have a few examples</li>\n",
    "    <ul>\n",
    "        <li>Think about a large dataset of plants, for rare species, we only have a few examples. </li>\n",
    "        <li>A database of user-searched keywords, some keywords might have been searched only 3,4 times</li>\n",
    "    </ul>\n",
    "</ul>\n",
    "<p>When we have a small dataset, the model does not see enough variations of data features during training. Therefore, there might exist many combinations that the model has not seen before and consequently, it cannot be generalized well to unseen data.</p>\n",
    "<p>One approach when dealing with the above issues is using pretrained networks. However, finetuning over a small dataset can impact the model accuracy and also lead to severe overfitting.</p>\n",
    "<p>Few-shot learning aims to provide a solution when having not large enough dataset. In few-shot learning, the goal is to learn how to learn a task rather than learning the task itself. From this perspective, few-shot learning is a type of meta-learning.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Meta Learning: Learning to Learn!</h2>\n",
    "<p>\n",
    "In traditional machine learning, a model is trained to perform a specific task, such as classifying images into a set of predefined categories. However, meta learning takes a different approach: it aims to teach the model how to learn, rather than just how to perform a specific task.\n",
    "\n",
    "To accomplish this, meta learning involves training the model on multiple episodes of related tasks. In each episode, the model is presented with a new classification task and must adapt its parameters and strategies to perform well on that task. By training on a diverse set of tasks, the model can learn to quickly adapt to new tasks in the future.\n",
    "\n",
    "Compared to traditional classification problems, meta learning typically involves working with sets of tasks rather than sets of data. This approach is often referred to as few-shot learning, as the model is trained to learn from only a few examples of each task.\n",
    "\n",
    "Overall, meta learning is a powerful approach to machine learning that can lead to more flexible and adaptable models. By teaching models how to learn, we can build AI systems that are better equipped to handle new challenges and tasks.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Few-shot learning explained</h2>\n",
    "<p>In a traditional classification problem, the model is trained to predict a class or label for a given sample image. During inference, the model can only classify images into the same set of classes that it was trained on, and cannot generalize to new classes. For example, if a model is trained to classify images as 'car' or 'airplane', it will not be able to classify a 'ship' image correctly, as it has not seen any 'ship' images before.\n",
    "\n",
    "Few-shot learning takes a different approach, aiming to enable the model to generalize to unseen classes. In a few-shot learning scenario, the model is trained to learn how to learn a task, rather than just memorizing specific classes. To achieve this, the model is trained on multiple small classification tasks, each with a few labeled samples. For instance, given a dataset of images of cats, dogs, horses, and sheep, one task could be to classify cats vs. dogs, another could be dogs vs. horses, and so on.\n",
    "\n",
    "By repeatedly training on a diverse set of classification tasks, the model learns to adapt to new tasks quickly and generalize to new classes. In the example above, if the model is trained on a few labeled images of ships, it should be able to classify a 'ship' image correctly, even though it was not explicitly trained on the 'ship' class.\n",
    "\n",
    "Overall, few-shot learning is a powerful technique for enabling models to learn how to learn and generalize to new classes, which can lead to more flexible and adaptable AI systems.\n",
    "\n",
    " </p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Few-shot learning setting: N-way-k-shot classification </h2>\n",
    "<p>\n",
    "    To train our model, we need to create episodes where it can learn to perform several tasks. Each task is essentially a small classification problem. To define a task, we first randomly sample $n$ classes (known as $n$-way) from our dataset. For example, let's say we choose the \"dog\" and \"cat\" classes from a set of samples that includes \"dog\", \"cat\", \"horse\", and \"sheep\". For each of the n classes, we then collect two sets of data: $k$ images to act as the training set (known as the support set), and $q$ images to act as the test set (known as the query set). \n",
    "    \n",
    "   \n",
    "<ul>\n",
    "    <li>$k$ number (k-shots) of images to play the role of the training set. This set of images is called support set.</li>\n",
    "    <li> $q$ number of images are sampled. This set of images is called query set. The query set plays the role of the test set.</li>\n",
    "</ul>\n",
    "</p>\n",
    "<p> Here are some notes to remember:\n",
    "    <ul>\n",
    "        <li>An episode consists of multiple tasks.</li>\n",
    "        <li>A few-shot learning problem with the above setting is known as $N$-way-$k$-shot classification.</li>\n",
    "        <li>The value of $k$ (the number of shots or the size of the support set) is typically small - for example, just 5 -  because our ultimate goal is for the model to be able to accurately predict the class (which has not seen during the training) using only a few labeled samples from the support set.</li> \n",
    "    </ul>\n",
    "     \n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
